{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8074ac69",
   "metadata": {},
   "source": [
    "# **Chnunking Strategies:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04070915",
   "metadata": {},
   "source": [
    "## **Different Types of Chunking Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae8a36",
   "metadata": {},
   "source": [
    "### **1. Fixed Size Chunking:**\n",
    "\n",
    "Fixed size chunking is a method of splitting documents into smaller chunks of a specified size, with optional overlap between chunks. This is useful when you want to process large documents in smaller, manageable pieces. Divides documents into equal-sized chunks based on a predefined metric, such as word or token count.â€‹ <br>\n",
    "\n",
    "* **How:** The text is split into segments of a specified size, regardless of the content's semantic boundaries.â€‹\n",
    "\n",
    "\n",
    "* **Example:** A 1,000-word document split into 100-word chunks results in 10 chunks.â€‹\n",
    "\n",
    "\n",
    "* **Pros:**\n",
    "    * Simple and quick to implement.\n",
    "    * Ensures uniform chunk sizes, facilitating consistent processing.â€‹\n",
    "\n",
    "\n",
    "* **Cons:**\n",
    "    * May split sentences or paragraphs, disrupting context.\n",
    "    * Not ideal for content requiring semantic coherence.â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72be2e",
   "metadata": {},
   "source": [
    "### **2. Document-Based Chunking:**\n",
    "\n",
    "Document chunking is a method of splitting documents into smaller chunks based on document structure like **`paragraphs`** and **`sections`**. It analyzes natural document boundaries rather than splitting at fixed character counts. This is useful when you want to process large documents while preserving semantic meaning and context. **Segments documents based on their `inherent structural elements`, such as `sections` or `headings`.** <br>\n",
    "\n",
    "* **How:** Utilizes the document's layout to determine chunk boundaries, preserving logical groupings.â€‹\n",
    "\n",
    "* **Example:** A research paper divided into chunks corresponding to its Abstract, Introduction, Methods, Results, and Conclusion sections.\n",
    "\n",
    "* **Pros:**\n",
    "    * Maintains the document's logical flow and context.\n",
    "    * Enhances retrieval relevance by aligning with natural divisions.\n",
    "\n",
    "* **Cons:**\n",
    "    * Requires documents to have clear structural markers.\n",
    "    * Less effective for unstructured or free-form text.â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6efb5e",
   "metadata": {},
   "source": [
    "### **3. Semantic Chunking:**\n",
    "\n",
    "**Semantic chunking is a method of splitting documents into smaller chunks by analyzing `semantic similarity` between text segments using `embeddings`**. It uses the chonkie library to identify natural **breakpoints where the semantic meaning changes significantly**, based on a configurable **similarity threshold**. This helps preserve context and meaning better than fixed-size chunking by ensuring semantically related content stays together in the same chunk, while splitting occurs at meaningful topic transitions. Divides text into chunks based on semantic meaning, ensuring each chunk contains a coherent idea. <br>\n",
    "\n",
    "\n",
    "* **How:** Employs natural language processing techniques to identify semantically related sentences or paragraphs.â€‹\n",
    "\n",
    "* **Example:** A news article segmented into chunks where each discusses a distinct aspect of the story.\n",
    "\n",
    "* **Pros:**\n",
    "    * Preserves meaning and context within chunks.\n",
    "    * Improves retrieval accuracy for semantically rich queries.\n",
    "\n",
    "* **Cons:**\n",
    "    * More computationally intensive than fixed-size methods.\n",
    "    * Complex to implement due to reliance on semantic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ecb98",
   "metadata": {},
   "source": [
    "### **4. Agentic Chunking:**\n",
    "\n",
    "\n",
    "**Agentic chunking is an intelligent method of splitting documents into smaller chunks by using an `LLM to determine natural breakpoints` in the text**. Rather than splitting text at fixed character counts, it analyzes the content to find semantically meaningful boundaries like paragraph breaks and topic transitions. Utilizes AI agents to dynamically determine chunk boundaries based on task-specific requirements. <br>\n",
    "\n",
    "* **How:** An AI agent analyzes the document, identifying and segmenting content into chunks optimized for specific tasks or queries.â€‹\n",
    "\n",
    "* **Example:** For a customer support chatbot, the agent segments a product manual into chunks corresponding to common user issues.\n",
    "\n",
    "* **Pros:**\n",
    "    * Highly adaptable to various tasks and contexts.\n",
    "    * Enhances relevance by aligning chunks with user intents.\n",
    "\n",
    "* **Cons:**\n",
    "    * Requires sophisticated AI models and training.\n",
    "    * Potentially resource-intensive and complex to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad22bb",
   "metadata": {},
   "source": [
    "### **5. Recursive Chunking:**\n",
    "\n",
    "Recursive chunking is a method of splitting documents into smaller chunks by recursively applying a chunking strategy. This is useful when you want to process large documents in smaller, manageable pieces.  <br>\n",
    "\n",
    "Recursive Chunking is a strategy that splits documents by trying a **`hierarchy of separators`** in a recursive manner. The goal is to preserve **`semantic meaning`** and **`structure`** while ensuring chunks stay within a specific token limit. <br>\n",
    "\n",
    "* **How It Works:** It recursively splits the document using a predefined list of separators like:\n",
    "    ```bash\n",
    "        [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    ```\n",
    "    1. Start with the largest separator (e.g., paragraphs: `\\n\\n`).\n",
    "    2. If a chunk is too large:\n",
    "        * Try a smaller separator (e.g., line break: `\\n`)\n",
    "    3. If still too large:\n",
    "        * Try sentence-level split (e.g., `\".\"`)\n",
    "    4. Continue down to character level if needed\n",
    "\n",
    "* **Why Use Recursive Chunking?**\n",
    "    * Maintains semantic boundaries (e.g., full paragraphs or sentences)\n",
    "    * Avoids breaking up ideas mid-sentence\n",
    "    * More natural for QA systems, chatbots, and search-based tasks\n",
    "\n",
    "* **Implementation:**\n",
    "    ```python\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,         # max tokens or characters per chunk\n",
    "            chunk_overlap=50,       # overlap for context continuity\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        chunks = splitter\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980c23d",
   "metadata": {},
   "source": [
    "## **Ideal Overlap Size When Chunking a Documents:**\n",
    "\n",
    "\n",
    "* **Overlap size** is the number of tokens (or characters) shared between adjacent chunks to maintain continuity of context. <br>\n",
    "\n",
    "* **General Recommendation:**\n",
    "    * Chunk size: `500â€“1,000` tokens\n",
    "    * Overlap size: `10â€“20%` of chunk size\n",
    "        * ðŸ‘‰ Typical value: `50â€“200` tokens\n",
    "\n",
    "* **Why Overlap is Important:**\n",
    "    * Ensures no important sentence or semantic content is cut off between chunks.\n",
    "    * Preserves context for models like GPT-4 to generate meaningful answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2d2c3",
   "metadata": {},
   "source": [
    "### **Calculate Total Token and Character Counts of a PDF Document:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c230f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b52030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 2111\n",
      "Total Tokens: 490\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "\n",
    "# Load PDF and extract all text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Count characters and tokens\n",
    "def get_token_character_counts(text, model=\"gpt-4\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return {\n",
    "        \"character_count\": len(text),\n",
    "        \"token_count\": len(tokens)\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"mypdf.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "stats = get_token_character_counts(text)\n",
    "\n",
    "print(f\"Total Characters: {stats['character_count']}\")\n",
    "print(f\"Total Tokens: {stats['token_count']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edc61b",
   "metadata": {},
   "source": [
    "## **Recommended Chunk Sizes (based on your case):**\n",
    "\n",
    "\n",
    "| **Use Case**                    | **Chunk Size**     | **Overlap**      | **Why**                                |\n",
    "|-----------------------------|----------------|--------------|------------------------------------|\n",
    "| FAQ-style Q&A               | 100 tokens     | 10 tokens    | Small content, fast retrieval     |\n",
    "| Chatbot-style RAG           | 150â€“200 tokens | 20â€“30 tokens | Balances granularity & context    |\n",
    "| Deep Document QA / Legal    | 300 tokens     | 50 tokens    | Keeps larger context together     |\n",
    "| Highly Semantic Search      | 100â€“150 tokens | 15â€“25 tokens | Allows precise embedding vectors |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc355c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d80f4a",
   "metadata": {},
   "source": [
    "## **References:**\n",
    "\n",
    "https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d\n",
    "\n",
    "https://docs.phidata.com/chunking/document-chunking\n",
    "\n",
    "https://www.ibm.com/think/tutorials/chunking-strategies-for-rag-with-langchain-watsonx-ai\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2024/10/chunking-techniques-to-build-exceptional-rag-systems/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4f8a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
