{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff4bd80",
   "metadata": {},
   "source": [
    "# **Create a Conversational ChtBot Using LangChain, OpenAI's Model and Various Prompting Techniques:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04180517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e060b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07250f63",
   "metadata": {},
   "source": [
    "## **Load LLM & Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f33864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI, OpenAIEmbeddings\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8f2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM:\n",
    "\n",
    "def load_llm(model_name:str=\"gpt-4o\", temperature: float=0.2, max_tokens:int=2000) -> OpenAI:\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=temperature, max_tokens=max_tokens)\n",
    "    return llm\n",
    "\n",
    "llm = load_llm(model_name=\"gpt-4o\", temperature=0.2, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cfb8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_17316073ab', 'id': 'chatcmpl-BMbiUFsXTIy57AJO2k5ePKYkHJYr7', 'finish_reason': 'stop', 'logprobs': None}, id='run-c7dd7962-e783-468d-be9c-399411557e4c-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e14cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings:\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76458a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92273c6",
   "metadata": {},
   "source": [
    "## **Vector Store:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa068514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07f0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever:\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=5), search_type=\"similarity\", search_score=True, search_score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7399791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cd89a678-5e7c-4b71-9d67-3a562638ab86', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(id='090b7a15-6294-4a43-a52c-6d59c28e3775', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       " Document(id='347ab430-0984-4847-8ca7-d0455a8a3958', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
       " Document(id='c16dba07-6b52-4115-a4c2-71d2fbb85932', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(id='4d97f9d7-e105-421f-8cdd-1963fc7fb1cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is the difference between LLMs and Agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca8b7b",
   "metadata": {},
   "source": [
    "## **Build Conversational AI Bot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1002dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3455bc0",
   "metadata": {},
   "source": [
    "### **Step 1: Create History-Aware-Retriever:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cec9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create History Aware Retriever:\n",
    "\n",
    "retriever_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n",
    "    \"formulate a standalone question which can be understood without the chat history.\"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "\n",
    "contextualize_q_prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", retriever_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "\n",
    "\n",
    "     ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e295dc",
   "metadata": {},
   "source": [
    "### **Step 2: Create Question-Aware-Chain Using Prompting Techniques:**\n",
    "\n",
    "1. System Prompt\n",
    "2. Few-Shot Prompt\n",
    "3. Chat-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17cd8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write System Messages:\n",
    "system_prompt = (\n",
    "    \"You are an AI Assistant named 'Lily,' specializing in AI and ML topics for students or users. \"\n",
    "    \"This AI Assistant is designed by Dibyendu Biswas, a Generative AI Engineer. \"\n",
    "    \"You provide clear, step-by-step explanations and encourage students to learn. \"\n",
    "    \"If a question is unclear, ask for clarification. \"\n",
    "    \"Use four sentences maximum and keep the answer concise. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Define Few-Shot Prompt Template using Few Examples:\n",
    "few_shot_examples = [\n",
    "    {'input': 'Hello', 'output': 'Hi there! How can I help you today?'},\n",
    "    {'input': 'What is your name?', 'output': 'I’m Lily, your assistant, specializing in AI and ML topics for students!'},\n",
    "    {'input': 'How are you?', 'output': 'I’m doing great, thanks for asking! How about you?'},\n",
    "    {'input': 'Tell me a joke', 'output': 'Why don’t skeletons fight each other? They don’t have the guts!'},\n",
    "    {'input': 'What time is it?', 'output': 'I’m not able to tell the time, but you can check your device!'},\n",
    "    {'input': 'Goodbye', 'output': 'Goodbye! Have a great day!'},\n",
    "    {'input': 'Can you help me?', 'output': 'Of course! What do you need help with?'},\n",
    "    {'input': 'What is the weather like?', 'output': 'I’m unable to check real-time weather, but you can look it up online!'},\n",
    "    {'input': 'Set a reminder', 'output': 'I can’t set reminders, but I can help you with other things!'},\n",
    "    {'input': 'Where are you from?', 'output': 'I don’t have a specific place, but I’m here to assist you wherever you are!'},\n",
    "    {'input': 'What can you do?', 'output': 'I can answer questions, tell jokes, help with tasks, and more! What do you need today?'},\n",
    "    {'input': 'Help me with math', 'output': 'Sure! What math problem would you like help with?'},\n",
    "    {'input': 'I am bored', 'output': 'Let’s do something fun! Want to hear a joke or maybe try a quiz?'},\n",
    "    {'input': 'What is 5 + 7?', 'output': '5 + 7 equals 12!'},\n",
    "    {'input': 'How do I cook pasta?', 'output': 'Boil water, add pasta, cook for 8-12 minutes, drain, and enjoy!'},\n",
    "    {'input': 'Tell me a fun fact', 'output': 'Did you know? A day on Venus is longer than a year on Venus!'},\n",
    "    {'input': 'What is your favorite color?', 'output': 'I don’t have a favorite color, but I think blue is nice!'}\n",
    "]\n",
    "\n",
    "# Few Shot Template:\n",
    "few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "# Few Shot Prompt:\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=few_shot_template,\n",
    "    examples=few_shot_examples,\n",
    ")\n",
    "\n",
    "# Define Final Prompt Using System prompt and Few Shot Prompt\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    few_shot_prompt,\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cdfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b4999",
   "metadata": {},
   "source": [
    "### **Step 3: Create Rag-Chain using history_aware_retriever and question_answer_chain:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "288ab4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rag-Chain using history_aware_retriever and question_answer_chain:\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d85e58",
   "metadata": {},
   "source": [
    "### **Step 4: Use the Memory and Session to store the current conversation:**\n",
    "\n",
    "* ChatMessageHistory,\n",
    "* BaseChatMessageHistory,\n",
    "* RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb8fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ea5fe",
   "metadata": {},
   "source": [
    "#### **Create a Session to Store Conversation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61422117",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082c8c7",
   "metadata": {},
   "source": [
    "#### **Create Conversational Rag Chain using `memory` and `rag_chain`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b13ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversational_rag_chain:\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36374892",
   "metadata": {},
   "source": [
    "## **Generate Response and store the current user session:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1160ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67fd8a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chain of Thought (CoT) prompting is a technique used in large language models to enhance reasoning by breaking down a problem into a sequence of intermediate steps. This approach helps the model to process complex tasks more effectively by generating a series of logical steps or thoughts, leading to a final solution. It improves the model's ability to handle tasks that require multi-step reasoning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Chain of Thought Prompt?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]\n",
    "\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed077082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Common ways to implement Chain of Thought prompting include: \n",
       "\n",
       "1. Simple prompting, such as asking the model to list steps for a task.\n",
       "2. Using task-specific instructions, like providing a story outline for writing.\n",
       "3. Incorporating human inputs to guide the decomposition of tasks into manageable steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans= conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]\n",
    "\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40073275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Chain of Thought Prompt?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Chain of Thought (CoT) prompting is a technique used in large language models to enhance reasoning by breaking down a problem into a sequence of intermediate steps. This approach helps the model to process complex tasks more effectively by generating a series of logical steps or thoughts, leading to a final solution. It improves the model's ability to handle tasks that require multi-step reasoning.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='Common ways to implement Chain of Thought prompting include: \\n\\n1. Simple prompting, such as asking the model to list steps for a task.\\n2. Using task-specific instructions, like providing a story outline for writing.\\n3. Incorporating human inputs to guide the decomposition of tasks into manageable steps.', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a841df97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Shyam! How can I assist you with your studies today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"My Name is Shyam and I am a student.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]\n",
    "\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeec92a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Shyam!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is my name?.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]\n",
    "\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "037d376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Chain of Thought Prompt?\n",
      "\n",
      "AI: Chain of Thought (CoT) prompting is a technique used in large language models to enhance reasoning by breaking down a problem into a sequence of intermediate steps. This approach helps the model to process complex tasks more effectively by generating a series of logical steps or thoughts, leading to a final solution. It improves the model's ability to handle tasks that require multi-step reasoning.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Common ways to implement Chain of Thought prompting include: \n",
      "\n",
      "1. Simple prompting, such as asking the model to list steps for a task.\n",
      "2. Using task-specific instructions, like providing a story outline for writing.\n",
      "3. Incorporating human inputs to guide the decomposition of tasks into manageable steps.\n",
      "\n",
      "User: What are the questions I asked before?\n",
      "\n",
      "AI: I'm unable to recall previous interactions or questions. However, feel free to ask anything again!\n",
      "\n",
      "User: My Name is Shyam and I am a student.\n",
      "\n",
      "AI: Nice to meet you, Shyam! How can I assist you with your studies today?\n",
      "\n",
      "User: What is my name?.\n",
      "\n",
      "AI: Your name is Shyam!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c41e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
