{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8e8866",
   "metadata": {},
   "source": [
    "# **Explore OpenAI:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb465a",
   "metadata": {},
   "source": [
    "## **Different Types of Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3f7d4",
   "metadata": {},
   "source": [
    "**1.** **Reasoning Models (o-series models):**\n",
    "* **What it is:** OpenAI‚Äôs `o-series` models (like `gpt-4o`) are optimized for advanced reasoning and complex, multi-step tasks. These models excel at logic, problem-solving, coding, and analytical work that requires sustained, coherent reasoning across multiple stages.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To tackle high-difficulty tasks like math problem-solving, scientific analysis, or multi-turn planning.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Solving difficult math or logic puzzles.\n",
    "    * Writing and debugging complex code.\n",
    "    * Planning multi-step tasks (e.g., business workflows).\n",
    "    * Conducting in-depth research and summarization.\n",
    "    * Supporting academic or technical writing.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**2.** **Flagship Chat Models:**\n",
    "* **What it is:** These are OpenAI‚Äôs **most advanced and general-purpose language models** (e.g., `gpt-4-turbo`). They are optimized for a broad range of chat-based tasks, combining high intelligence, fluency, and versatility.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To serve as all-purpose AI assistants that perform well in most use cases.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * General conversation and Q&A.\n",
    "    * Drafting emails, documents, or content.\n",
    "    * Language translation.\n",
    "    * Brainstorming ideas.\n",
    "    * Tutoring and education.\n",
    "    * Coding help and debugging.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**3.** **Cost-Optimized Models:**\n",
    "* **What it is:** These models (like `gpt-3.5-turbo`) are **smaller**, **faster**, and **less expensive** to run than the flagship models, making them ideal for high-volume or latency-sensitive applications.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To provide a balance between performance and cost-efficiency for less complex tasks.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Customer service chatbots.\n",
    "    * FAQ and knowledgebase bots.\n",
    "    * Drafting simple content.\n",
    "    * Data entry assistance.\n",
    "    * Repetitive or low-complexity tasks at scale.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**4.** **Realtime Models:**\n",
    "* **What it is:** These models are optimized for **low-latency** responses across **text and audio inputs/outputs**, including real-time conversation and voice interaction.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To support interactive, real-time applications where speed and responsiveness are critical.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Voice assistants and AI agents.\n",
    "    * Live conversation tools.\n",
    "    * Real-time language translation.\n",
    "    * Interactive apps (games, education, etc.).\n",
    "    * Accessibility tools (e.g., reading support).\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**5.** **Text-to-Speech (TTS):**\n",
    "* **What it is:** OpenAI‚Äôs TTS models convert text into **natural-sounding spoken audio** in various voices and styles (including `tts-1`, `tts-1-hd`).\n",
    "\n",
    "* **Purpose:**\n",
    "    * To bring voice capabilities to apps and tools for enhanced communication and accessibility.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Voice assistants.\n",
    "    * Audiobook and podcast generation.\n",
    "    * Voiceovers for videos or games.\n",
    "    * Screen readers for accessibility.\n",
    "    * Personalized spoken messages.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**6.** **Transcription (Whisper):**\n",
    "* **What it is:** The whisper model is used for **transcribing** and **translating audio to text**. It supports many languages and is known for high accuracy.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To convert spoken language into written form for accessibility, indexing, and analysis.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Meeting and interview transcription.\n",
    "    * Subtitle generation for videos.\n",
    "    * Voice note conversion.\n",
    "    * Multilingual translation of audio content.\n",
    "    * Call center analysis.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**7.** **Embeddings:**\n",
    "* **What it is:** Embedding models (like `text-embedding-3-small` and `text-embedding-3-large`) convert text into **vector representations** that capture meaning and context, used for comparing or clustering language data.\n",
    "\n",
    "* **Purpose:**\n",
    "    * To enable semantic understanding and comparison of text in machine learning systems.\n",
    "\n",
    "* **Use Cases:**\n",
    "    * Semantic search.\n",
    "    * Document similarity detection.\n",
    "    * Clustering and categorization.\n",
    "    * Personalized recommendations.\n",
    "    * Text classification or retrieval.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**üß† OpenAI Model Types Summary:**\n",
    "\n",
    "| **Type**                     | **What It Is**                                                                 | **Purpose**                                          | **Common Use Cases**                                                                 |\n",
    "|------------------------------|---------------------------------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| **Reasoning Models** (e.g., gpt-4o)     | Advanced models for complex, multi-step reasoning.                                | High-accuracy logic, planning, and technical tasks. | Math & logic problems, research, coding, complex planning, deep analysis.           |\n",
    "| **Flagship Chat Models** (e.g., gpt-4-turbo) | Versatile, high-performing general AI chat models.                              | All-purpose assistant for varied tasks.             | Q&A, creative writing, tutoring, translation, ideation, productivity help.          |\n",
    "| **Cost-Optimized Models** (e.g., gpt-3.5-turbo) | Faster, cheaper models for lightweight tasks.                                   | Lower-cost options for large-scale or fast interactions. | Chatbots, simple drafting, low-complexity workflows, support tools.                |\n",
    "| **Realtime Models**                 | Models designed for low-latency, real-time interaction in text/audio.            | Enable fast, interactive experiences.               | Voice assistants, live translation, games, education tools, instant response apps.  |\n",
    "| **Text-to-Speech** (e.g., tts-1, tts-1-hd) | Converts text into realistic spoken audio.                                       | Give apps a natural, human-like voice.              | Audiobooks, screen readers, voiceovers, custom assistant voices, accessibility.     |\n",
    "| **Transcription** (Whisper)         | Transcribes & translates audio into text with multilingual support.              | Convert speech to text accurately and quickly.      | Meetings, subtitles, dictation, multilingual transcription, call center analysis.   |\n",
    "| **Embeddings** (e.g., text-embedding-3-small) | Turns text into numerical vectors that capture meaning/context.                  | Enable machines to understand and compare language. | Semantic search, document similarity, clustering, recommendations, NLP pipelines.  |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "**üîç Comparison: Reasoning vs Flagship Chat vs Cost-Optimized vs Realtime Models:**\n",
    "| **Feature**                   | **Reasoning Models** (e.g., gpt-4o)               | **Flagship Chat Models** (e.g., gpt-4-turbo)       | **Cost-Optimized Models** (e.g., gpt-3.5-turbo)    | **Realtime Models**                              |\n",
    "|-------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|--------------------------------------------------|\n",
    "| **Primary Focus**              | Advanced reasoning, multi-step logic              | Versatility and intelligence in broad tasks         | Speed and cost-efficiency for basic tasks           | Instant responses with low latency, especially in audio |\n",
    "| **Performance Level**          | Highest for complex tasks                         | Very high, general-purpose                         | Moderate, optimized for simplicity                 | High-speed for real-time use cases               |\n",
    "| **Speed**                      | Fast, but prioritizes depth over latency          | Balanced                                           | Fast and lightweight                                | Ultra-fast and responsive                        |\n",
    "| **Cost**                       | Mid to high                                       | Medium                                             | Low                                                | Varies (depends on use and mode)                 |\n",
    "| **Multimodal Capabilities**    | Yes (text, vision, audio input/output)            | Yes (text and image input)                          | Text-only                                          | Text and audio I/O in real time                  |\n",
    "| **Best For**                   | Technical work, research, coding, logic           | Chatbots, assistants, writing, education           | High-volume apps, simple bots, drafting            | Voice assistants, live interactions, accessibility |\n",
    "| **Example Use Cases**          | Solving math problems, writing code, strategic planning | Chat-based tutoring, writing, customer support      | FAQ bots, fast content generation, lightweight tools | Voice UX, real-time translation, interactive AI experiences |\n",
    "| **Model Examples**             | gpt-4o (optimized for reasoning)                  | gpt-4-turbo                                        | gpt-3.5-turbo                                      | gpt-4o in real-time use cases                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5091e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bd60ae",
   "metadata": {},
   "source": [
    "## **Model Wise Information:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d72805",
   "metadata": {},
   "source": [
    "**Key Concepts:** <br>\n",
    "* **Context Window:** The total number of **tokens** (words, parts of words, punctuation, etc.) the model can \"see\" at once ‚Äî including both the input prompt and the output. It determines how much information the model can consider in a single request.\n",
    "\n",
    "* **Max Output Tokens:** The maximum number of **tokens the model can generate in its response**. It defines the longest possible output the model can produce in one completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de49e5a",
   "metadata": {},
   "source": [
    "**1.** **Reasoning models:**\n",
    "* **o3-mini:**\n",
    "    * **Descriptions:** o3-mini is our newest small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini. o3-mini supports key developer features, like Structured Outputs, function calling, and Batch API.\n",
    "    * **Context Window:** 200,000\n",
    "    * **Output Tokens:** 100,000\n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:**  $1.10\n",
    "        * **Cached:** $0.55\n",
    "        * **Output:** $4.40\n",
    "\n",
    "* **o1:**\n",
    "    * **Descriptions:** The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.\n",
    "    * **Context Window:** 200,000 \n",
    "    * **Output Tokens:** 100,000\n",
    "    * **Price per 1M Request:**\n",
    "        * **Input:** $15.00\n",
    "        * **Cached:** $7.50\n",
    "        * **Output:** $60.00\n",
    "\n",
    "* **o1-mini:**\n",
    "    * **Descriptions:** The o1 reasoning model is designed to solve hard problems across domains. o1-mini is a faster and more affordable reasoning model, but we recommend using the newer o3-mini model that features higher intelligence at the same latency and price as o1-mini.\n",
    "    * **Context Window:** 128,000 \n",
    "    * **Output Tokens:** 65,536\n",
    "    * **Price per 1M Request:**\n",
    "        * **Input:** $1.10\n",
    "        * **Cached:** $0.55\n",
    "        * **Output:** $4.40\n",
    "\n",
    "* **o1-pro:**\n",
    "    * **Descriptions:** The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers. o1-pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests, and other advanced API features in the future.\n",
    "    * **Context Window:** 200,000\n",
    "    * **Output Tokens:** 100,000\n",
    "    * **Price per 1M Request:**\n",
    "        * **Input:** $75.00\n",
    "        * **Output:** $300.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7a877",
   "metadata": {},
   "source": [
    "**2.** **Flagship chat models:**\n",
    "* **GPT-4.1:**\n",
    "    * **Descriptions:** GPT-4.1 is our flagship model for complex tasks. It is well suited for problem solving across domains.\n",
    "    * **Context Window:** 1,047,576\n",
    "    * **Output Tokens:** 32,768\n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $2.00\n",
    "        * **Cached:** $0.50\n",
    "        * **Output:** $8.00\n",
    "\n",
    "* **GPT-4o:**\n",
    "    * **Descriptions:** GPT-4o (‚Äúo‚Äù for ‚Äúomni‚Äù) is our versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). It is the best model for most tasks, and is our most capable model outside of our o-series models.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 16,384 \n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $2.50\n",
    "        * **Cached:** $1.25\n",
    "        * **Output:** $10.00\n",
    "\n",
    "* **GPT-4o Audio:**\n",
    "    * **Descriptions:** This is a preview release of the GPT-4o Audio models. These models accept audio inputs and outputs, and can be used in the Chat Completions REST API.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 16,384\n",
    "    * **Price per 1M Request Text:** \n",
    "        * **Input:** $2.50\n",
    "        * **Output:** $10.00\n",
    "    * **Price per 1M Request Audio:** \n",
    "        * **Input:** $40.00\n",
    "        * **Output:** $80.00\n",
    "\n",
    "* **ChatGPT-4o:**\n",
    "    * **Descriptions:** ChatGPT-4o points to the GPT-4o snapshot currently used in ChatGPT. GPT-4o is our versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs. It is the best model for most tasks, and is our most capable model outside of our o-series models.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 16,384\n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $5.00\n",
    "        * **Output:** $15.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ac389",
   "metadata": {},
   "source": [
    "**2.** **Cost-optimized models:**\n",
    "* **GPT-4.1 mini:**\n",
    "    * **Descriptions:** GPT-4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.\n",
    "    * **Context Window:** 1,047,576 \n",
    "    * **Output Tokens:** 32,768\n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $0.40\n",
    "        * **Cached:** $0.10\n",
    "        * **Output:** $1.60\n",
    "\n",
    "* **GPT-4.1 nano:**\n",
    "    * **Descriptions:** GPT-4.1 nano is the fastest, most cost-effective GPT-4.1 model.\n",
    "    * **Context Window:** 1,047,576\n",
    "    * **Output Tokens:** 32,768 \n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $0.10\n",
    "        * **Cached:** $0.025\n",
    "        * **Output:** $0.40\n",
    "\n",
    "* **GPT-4o mini:**\n",
    "    * **Descriptions:** GPT-4o mini (‚Äúo‚Äù for ‚Äúomni‚Äù) is a fast, affordable small model for focused tasks. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). It is ideal for `fine-tuning`, and model outputs from a larger model like GPT-4o can be distilled to GPT-4o-mini to produce similar results at lower cost and latency.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 16,384 \n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $0.15\n",
    "        * **Cached:** $0.075\n",
    "        * **Output:** $0.60\n",
    "\n",
    "* **GPT-4o mini Audio:**\n",
    "    * **Descriptions:** This is a preview release of the smaller GPT-4o Audio mini model. It's designed to input audio or create audio outputs via the REST API.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 16,384 \n",
    "    * **Price per 1M Request:** \n",
    "        * **Input:** $0.15\n",
    "        * **Output:** $0.60\n",
    "    * **Price for 1M Request Audio tokens:**\n",
    "        * **Input:** $10.00\n",
    "        * **Output:** $20.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca6e40",
   "metadata": {},
   "source": [
    "**3.** **Realtime models:**\n",
    "* **GPT-4o Realtime:**\n",
    "    * **Descriptions:** This is a preview release of the GPT-4o Realtime model, capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 4,096\n",
    "    * **Price per 1M Request Text:** \n",
    "        * **Input:** $5.00\n",
    "        * **Cached:** $2.50\n",
    "        * **Output:** $20.00\n",
    "    * **Price per 1M Request Audio:** \n",
    "        * **Input:** $40.00\n",
    "        * **Cached:** $2.50\n",
    "        * **Output:** $80.00\n",
    "\n",
    "* **GPT-4o mini Realtime:**\n",
    "    * **Descriptions:** This is a preview release of the GPT-4o-mini Realtime model, capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface.\n",
    "    * **Context Window:** 128,000\n",
    "    * **Output Tokens:** 4,096\n",
    "    * **Price per 1M Request Text:** \n",
    "        * **Input:** $0.60\n",
    "        * **Cached:** $0.30\n",
    "        * **Output:** $2.40\n",
    "    * **Price per 1M Request Audio:** \n",
    "        * **Input:** $10.00\n",
    "        * **Cached:** $0.30\n",
    "        * **Output:** $20.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d844",
   "metadata": {},
   "source": [
    "**4.** **Text-to-speech models:**\n",
    "* **GPT-4o mini TTS:**\n",
    "    * **Descriptions:** GPT-4o mini TTS is a text-to-speech model built on GPT-4o mini, a fast and powerful language model. Use it to convert text to natural sounding spoken text. The maximum number of input tokens is 2000.\n",
    "    * **Context Window:** 2000\n",
    "    * **Output Tokens:** None, i.e. Audio\n",
    "    * **Price per 1M Request Text:** \n",
    "        * **Input:** $0.60\n",
    "    * **Price per 1M Request Audio:** \n",
    "        * **Output:** $12.00\n",
    "\n",
    "* **TTS-1:**\n",
    "    * **Descriptions:** TTS is a model that converts text to natural sounding spoken text. The tts-1 model is optimized for realtime text-to-speech use cases. Use it with the Speech endpoint in the Audio API.\n",
    "    * **Cost for 1M Request:** $15.00\n",
    "    * **Use Cases:** Speech generation\n",
    "\n",
    "* **TTS-1 HD:**\n",
    "    * **Descriptions:** TTS is a model that converts text to natural sounding spoken text. The tts-1-hd model is optimized for high quality text-to-speech use cases. Use it with the Speech endpoint in the Audio API.\n",
    "    * **Cost for 1M Request:** $30.00\n",
    "    * **Use Cases:** Speech generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac7be4",
   "metadata": {},
   "source": [
    "**5.** **Embeddings:**\n",
    "* **text-embedding-3-small:**\n",
    "    * **Descriptions:** text-embedding-3-small is our improved, more performant version of our ada embedding model. Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
    "    * **Speed:** Medium\n",
    "    * **Embedding Dimension:** 1,536\n",
    "    * **Cost per 1M Request:** $0.01\n",
    "\n",
    "* **text-embedding-3-large:**\n",
    "    * **Descriptions:** text-embedding-3-large is our most capable embedding model for both english and non-english tasks. Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
    "    * **Speed:** Slow\n",
    "    * **Embedding Dimension:** 3,072\n",
    "    * **Cost per 1M Request:** $0.13\n",
    "\n",
    "* **text-embedding-ada-002:**\n",
    "    * **Descriptions:** text-embedding-ada-002 is our improved, more performant version of our ada embedding model. Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
    "    * **Speed:** slow\n",
    "    * **Embedding Dimension:** 1,536\n",
    "    * **Cost per 1M Request:** $0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b9f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
